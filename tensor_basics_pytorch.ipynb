{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# III. Tensor Basics in PyTorch\n",
    "\n",
    "#### Intro to Tensors\n",
    "\n",
    "##### Understanding Tensors\n",
    "Tensors form the backbone of any machine learning or deep learning system. They are a mathematical entity that lives in a structure and interacts with other mathematical entities. If you've ever performed calculations with scalars, vectors, or matrices, then you've worked with special cases of tensors.\n",
    "\n",
    "Tensors are a generalization of scalars, vectors, and matrices to an arbitrary number of dimensions. They are containers for data â€“ almost always numerical data. So, they're a container for numbers. You can think of tensors as multi-dimensional arrays.\n",
    "\n",
    "##### How are Tensors Different from Vectors and Matrices?\n",
    "Scalars, vectors, and matrices are all special cases of tensors.\n",
    "\n",
    "* A scalar is a single number, or you can say it is a 0-dimensional tensor. For example, \"7\" is a scalar value.\n",
    "\n",
    "* A vector is an array of numbers, or a 1-dimensional tensor. For example, [1, 2, 3] is a 3-dimensional vector.\n",
    "\n",
    "* A matrix is a 2-dimensional grid of numbers, or a 2D tensor. For example, [[1, 2, 3], [4, 5, 6], [7, 8, 9]] is a 3x3 matrix.\n",
    "\n",
    "So, a tensor that contains only one number is a scalar, a tensor with three numbers is a vector, and a tensor with two axes is a matrix. Tensors are defined by three key attributes:\n",
    "\n",
    "* Number of axes (rank): For instance, a 3D tensor has three axes, and a matrix has two axes.\n",
    "\n",
    "* Shape: This is a tuple of integers that describes how many dimensions the tensor has along each axis. For example, a matrix's shape might be (3, 5).\n",
    "\n",
    "* Data type (dtype): This is the type of data contained in the tensor; for example, the tensor's type could be float32, uint8, float64, and so on.\n",
    "\n",
    "##### Tensors in the Context of Neural Networks\n",
    "Neural networks take tensors as input, process these tensors using layers (which are also represented as tensors), and output tensors. Here's why tensors are important in the context of neural networks:\n",
    "\n",
    "* Efficiency and Speed: Tensors are a vital aspect in the field of deep learning algorithms due to the possibility of performing parallel operations on them, which leads to a performance gain. Modern hardware accelerators like GPUs allow tensor operations to be massively parallelized, and frameworks like PyTorch and TensorFlow are designed to efficiently work with tensors on these hardware accelerators, leading to efficient computation even for large-scale neural networks.\n",
    "\n",
    "* Flexibility: Tensors are highly flexible and can represent a wide range of data. For instance, color images can be represented as 3D tensors. The dimensions correspond to the height, width, and color depth. Even more complex data structures can be represented as tensors. For instance, videos can be represented as 4D tensors, where dimensions correspond to frame index, height, width, and color depth.\n",
    "\n",
    "* Support for Automatic Differentiation: Deep learning frameworks that use tensors also provide automatic differentiation and gradient calculation, which are invaluable for backpropagation in neural networks. This feature simplifies the implementation of new models and the computation of gradients.\n",
    "\n",
    "* Scalability and Batch Processing: Tensors enable batch processing of data, which is crucial in training deep learning models. Instead of passing a single data point (scalar) or a single data sample (vector), we can process a batch of data in parallel, leading to greater utilization of the GPU and faster training.\n",
    "\n",
    "Tensors are fundamental to the operation of neural networks. Their high dimensionality allows for the encapsulation of complex data structures, while their efficient implementation allows for fast computation and suitability for gradient-based optimization algorithms, like those used in training deep learning models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating and manipulating Tensors in PyTorch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with a single number\n",
    "t1 = torch.tensor(4.)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 2., 3., 4.])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a vector with more numbers\n",
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "t2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5.,  6.],\n        [ 7.,  8.],\n        [ 9., 10.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a matrix\n",
    "t3 = torch.tensor([[5., 6], [7, 8], [9, 10]])\n",
    "t3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[11., 12., 13.],\n         [13., 14., 15.]],\n\n        [[15., 16., 17.],\n         [17., 18., 19.]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a 3-dimensional tensor\n",
    "t4 = torch.tensor([\n",
    "    [[11, 12, 13],\n",
    "     [13, 14, 15]],\n",
    "    [[15, 16, 17],\n",
    "     [17, 18, 19.]]])\n",
    "t4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([4])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# check the shape of tensors\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)\n",
    "print(t4.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensor Operations and Gradients\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  8.],\n",
      "        [ 9., 10.],\n",
      "        [11., 12.]])\n",
      "tensor([[10., 12.],\n",
      "        [14., 16.],\n",
      "        [18., 20.]])\n",
      "tensor([[16.],\n",
      "        [22.],\n",
      "        [28.]])\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.],\n",
      "        [14., 16.]])\n",
      "tensor([[ 5.,  6.,  7.,  8.,  9., 10.]])\n"
     ]
    }
   ],
   "source": [
    "# addition\n",
    "print(t3 + 2)\n",
    "\n",
    "# multiplication\n",
    "print(t3 * 2)\n",
    "\n",
    "# matrix multiplication\n",
    "print(t3 @ torch.tensor([[2.], [1.]]))\n",
    "\n",
    "# add 2 tensors\n",
    "t5 = torch.tensor([[1., 2], [3, 4], [5, 6]])\n",
    "print(t3 + t5)\n",
    "\n",
    "# reshape a tensor\n",
    "print(t3.reshape(1, 6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tensor Gradients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 8., 11., 14.],\n",
      "         [17., 24., 31.]],\n",
      "\n",
      "        [[20., 23., 26.],\n",
      "         [45., 52., 59.]]], grad_fn=<AddBackward0>)\n",
      "tensor([[24., 36.],\n",
      "        [24., 36.]])\n",
      "tensor([[6.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "source": [
    "# create a 3D tensor with gradients enabled\n",
    "t6 = torch.tensor([\n",
    "    [[1., 2, 3],\n",
    "     [3, 4, 5]],\n",
    "    [[5, 6, 7],\n",
    "     [7, 8, 9.]]])\n",
    "\n",
    "w = torch.tensor([[1., 2], [3, 4]], requires_grad=True)\n",
    "b = torch.tensor([[1.], [2]], requires_grad=True)\n",
    "\n",
    "# compute the model output\n",
    "y = w @ t6 + b\n",
    "print(y)\n",
    "\n",
    "# compute gradients\n",
    "y.backward(torch.ones_like(y))\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
